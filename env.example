# LLM proxy configuration (required for code review)
# Supply these at image/compose build time (e.g. in .env when running docker compose build)
# See README for runtime-override and security options.

# Your LLM proxy base URL (OpenAI-compatible, e.g. https://your-proxy.example.com/v1)
LLM_BASE_URL=https://your-llm-proxy.example.com

# API key for your LLM proxy (sk-KEY; provided at build time as requested)
LLM_API_KEY=sk-key

# Model name to use (e.g. gpt-4, claude-3-sonnet, vertex_ai/claude-sonnet-4-5 or your proxy's model id)
# For llm proxy sometims prefix is needed when use LiteLLM 'litellm_proxy/your/model'
LLM_MODEL=gpt-4

# Project directory to mount (absolute path)
# This is the directory containing code you want to review
PROJECT_DIR=/path/to/your/project
